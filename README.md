# Raw-Telegram-Data-to-an-Analytical-API
ğŸ¥ Shipping a Data Product: From Raw Telegram Data to an Analytical API
ğŸ“Œ Project Overview

This project builds an end-to-end, production-ready data pipeline that transforms raw, unstructured Telegram data into a clean, enriched, and queryable analytical API.

The system focuses on Ethiopian medical and pharmaceutical Telegram channels, extracting insights about products, prices, posting behavior, and visual content. It follows modern data engineering best practices, including ELT architecture, dimensional modeling, data validation, enrichment with computer vision, and pipeline orchestration.

The final result is not just data â€” but a reliable data product.

ğŸ¯ Business Problem

Medical businesses in Ethiopia actively use Telegram to advertise products, prices, and availability. However:

The data is scattered across channels

Messages are unstructured and inconsistent

Images contain valuable signals that are often ignored

Manual analysis does not scale

This project answers questions such as:

What are the most frequently mentioned medical products?

How does activity vary across channels and time?

Which channels rely more on visual promotion?

Do posts with people or product images attract more attention?

ğŸ—ï¸ Architecture Overview

The project follows a layered ELT architecture:

Telegram â†’ Data Lake â†’ PostgreSQL (Raw) â†’ dbt (Staging & Marts)
        â†’ YOLO Image Enrichment â†’ Analytical API â†’ Orchestration (Dagster)


Each layer has a clear responsibility, making the system robust, scalable, and reproducible.

ğŸ“ Project Structure
medical-telegram-warehouse/
â”‚
â”œâ”€â”€ src/                    # Core pipeline scripts
â”‚   â”œâ”€â”€ scraper.py          # Telegram scraping
â”‚   â”œâ”€â”€ load_raw_to_postgres.py
â”‚   â””â”€â”€ yolo_detect.py      # Image enrichment
â”‚
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ database.py
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ medical_warehouse/      # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â””â”€â”€ tests/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # JSON + images
â”‚   â””â”€â”€ processed/          # YOLO outputs
â”‚
â”œâ”€â”€ pipeline.py             # Dagster orchestration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env                    # Secrets (not committed)
â””â”€â”€ README.md

âœ… Task Breakdown
ğŸ”¹ Task 1 â€“ Data Scraping & Loading (Extract & Load)
What was done

Connected securely to the Telegram API using Telethon

Scraped a controlled number of messages from public medical channels

Extracted:

Message ID, timestamp, text

Views and forwards

Media metadata

Downloaded images into a structured folder hierarchy

Stored raw data as partitioned JSON files

Implemented logging for observability

Loaded raw JSON data into PostgreSQL without modification

Result

A trustworthy raw data lake and a raw.telegram_messages table preserving original truth.

ğŸ”¹ Task 2 â€“ Data Modeling & Transformation (Transform)
What was done

Initialized a dbt project connected to PostgreSQL

Created schemas:

raw â†’ original data

staging â†’ cleaned data

marts â†’ analytics-ready models

Built staging models to:

Cast data types

Standardize column names

Remove invalid records

Add derived fields

Designed a star schema:

dim_channels

dim_dates

fct_messages

Implemented dbt tests:

not_null, unique

Foreign key relationships

Custom business rule tests

Generated dbt documentation

Result

A clean, tested, and documented data warehouse optimized for analytics and APIs.

ğŸ”¹ Task 3 â€“ Data Enrichment with YOLOv8 (Enrich)
What was done

Used YOLOv8 nano for efficient object detection

Scanned downloaded Telegram images

Detected objects and confidence scores

Classified images into:

promotional

product_display

lifestyle

other

Stored results in a CSV

Integrated image data into the warehouse via dbt

Result

Unstructured images were transformed into structured analytical signals, enabling visual-content insights.

ğŸ”¹ Task 4 â€“ Analytical API (Serve)
What was done

Built a FastAPI application

Connected to PostgreSQL via SQLAlchemy

Implemented analytical endpoints:

Top mentioned products

Channel activity trends

Message keyword search

Visual content statistics

Added Pydantic schemas for validation

Enabled automatic OpenAPI documentation

Result

A self-documenting analytical API that exposes warehouse insights to dashboards and users.

ğŸ”¹ Task 5 â€“ Pipeline Orchestration with Dagster (Automate)
What was done

Converted each pipeline step into Dagster ops

Defined a job enforcing execution order

Enabled logging, retries, and observability

Configured daily scheduling

Verified execution via Dagster UI

Result

The pipeline became a fully automated, observable workflow, no longer a collection of scripts.

ğŸ” Reproducibility & Reliability

Reproducibility is ensured through:

.env for secrets and credentials

requirements.txt for dependency control

dbt for deterministic transformations

Dagster for execution guarantees

Clear module boundaries

To reproduce the system:

Set environment variables

Install dependencies

Run Dagster or individual components

ğŸš€ How to Run
pip install -r requirements.txt
dagster dev -f pipeline.py


Access:

Dagster UI â†’ http://localhost:3000

API Docs â†’ http://localhost:8000/docs

ğŸ”® Future Extensions

Product-level entity recognition (NER)

Price extraction using NLP

Alerting on product availability changes

Dashboard integration (Superset / Power BI)

Domain-specific computer vision models